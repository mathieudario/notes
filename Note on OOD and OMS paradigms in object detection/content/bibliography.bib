@misc{guerin2023,
    title={Out-{Of}-{Distribution} {Detection} {Is} {Not} {All} {You} {Need}},
    doi={10.48550/arXiv.2211.16158},
    url={http://arxiv.org/abs/2211.16158},
    urldate={2025-01-14},
    abstract={The usage of deep neural networks in safety-critical systems is limited by our ability to guarantee their correct behavior. Runtime monitors are components aiming to identify unsafe predictions and discard them before they can lead to catastrophic consequences. Several recent works on runtime monitoring have focused on out-of-distribution (OOD) detection, i.e., identifying inputs that are different from the training data. In this work, we argue that OOD detection is not a well-suited framework to design efficient runtime monitors and that it is more relevant to evaluate monitors based on their ability to discard incorrect predictions. We call this setting out-ofmodel-scope detection and discuss the conceptual differences with OOD. We also conduct extensive experiments on popular datasets from the literature to show that studying monitors in the OOD setting can be misleading: 1. very good OOD results can give a false impression of safety, 2. comparison under the OOD setting does not allow identifying the best monitor to detect errors. Finally, we also show that removing erroneous training data samples helps to train better monitors.},
    publisher={arXiv},
    author={Guérin, Joris and Delmas, Kevin and Ferreira, Raul Sena and Guiochet, Jérémie},
    year={2023},
    month=jan,
    note={arXiv:2211.16158 [cs]},
    keywords={Monitoring Benchmark, OOD vs OMS, Monitors taxonomy, Monitors evaluation workflow, Monitors comparison, Data-based monitoring},
}

@misc{bolya_2020,
	title = {{TIDE}: {A} {General} {Toolbox} for {Identifying} {Object} {Detection} {Errors}},
	shorttitle = {{TIDE}},
	url = {http://arxiv.org/abs/2008.08115},
	doi = {10.48550/arXiv.2008.08115},
	abstract = {We introduce TIDE, a framework and associated toolbox for analyzing the sources of error in object detection and instance segmentation algorithms. Importantly, our framework is applicable across datasets and can be applied directly to output prediction files without required knowledge of the underlying prediction system. Thus, our framework can be used as a drop-in replacement for the standard mAP computation while providing a comprehensive analysis of each model's strengths and weaknesses. We segment errors into six types and, crucially, are the first to introduce a technique for measuring the contribution of each error in a way that isolates its effect on overall performance. We show that such a representation is critical for drawing accurate, comprehensive conclusions through in-depth analysis across 4 datasets and 7 recognition models. Available at https://dbolya.github.io/tide/},
	urldate = {2025-02-13},
	publisher = {arXiv},
	author = {Bolya, Daniel and Foley, Sean and Hays, James and Hoffman, Judy},
	month = aug,
	year = {2020},
	note = {arXiv:2008.08115 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Updated LVIS results with the v1.0.1 error calculation},
	file = {Preprint PDF:C\:\\Users\\MathieuDARIO\\Zotero\\storage\\6FTAS3Z2\\Bolya et al. - 2020 - TIDE A General Toolbox for Identifying Object Detection Errors.pdf:application/pdf;Snapshot:C\:\\Users\\MathieuDARIO\\Zotero\\storage\\JLRFSYK2\\2008.html:text/html},
}

@misc{ducoffe_2023,
	title = {{LARD} -- {Landing} {Approach} {Runway} {Detection} -- {Dataset} for {Vision} {Based} {Landing}},
	url = {http://arxiv.org/abs/2304.09938},
	doi = {10.48550/arXiv.2304.09938},
	abstract = {As the interest in autonomous systems continues to grow, one of the major challenges is collecting sufficient and representative real-world data. Despite the strong practical and commercial interest in autonomous landing systems in the aerospace field, there is a lack of open-source datasets of aerial images. To address this issue, we present a dataset-lard-of high-quality aerial images for the task of runway detection during approach and landing phases. Most of the dataset is composed of synthetic images but we also provide manually labelled images from real landing footages, to extend the detection task to a more realistic setting. In addition, we offer the generator which can produce such synthetic front-view images and enables automatic annotation of the runway corners through geometric transformations. This dataset paves the way for further research such as the analysis of dataset quality or the development of models to cope with the detection tasks. Find data, code and more up-to-date information at https://github.com/deel-ai/LARD},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Ducoffe, Mélanie and Carrere, Maxime and Féliers, Léo and Gauffriau, Adrien and Mussot, Vincent and Pagetti, Claire and Sammour, Thierry},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09938 [cs]},
	keywords = {object detection, LARD, semantic segmentation, dataset CV},
}

